"""
Solver Agent for Multi-Agent Evolve Co-Evolution System

Based on arXiv:2510.23595 "Multi-Agent Evolve: LLM Self-Improve through Co-evolution"
Research: docs/research/MULTI_AGENT_EVOLVE_ARCHITECTURE.md

The Solver generates solution trajectories with focus on:
- Diversity (avoid local minima)
- Quality (benchmark performance)
- Verifier-aware generation (anticipate verification challenges)

Co-Evolution Objective:
    reward = quality_weight * benchmark_score
           + diversity_weight * diversity_score
           + verifier_weight * verifier_difficulty_score

Integration Points:
- SE-Darwin operators (Revision, Recombination, Refinement)
- TrajectoryPool (persistent trajectory storage)
- BenchmarkRunner (empirical validation)
- OTEL observability (distributed tracing)

Author: Hudson (Implementation Specialist)
Date: November 3, 2025
Status: Phase 2 Implementation
"""

import asyncio
import hashlib
import json
import logging
import time
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any, Tuple, Set

# Genesis infrastructure imports
from infrastructure import get_logger
from infrastructure.trajectory_pool import (
    Trajectory,
    TrajectoryPool,
    TrajectoryStatus,
    OperatorType,
    get_trajectory_pool
)
from infrastructure.se_operators import (
    RevisionOperator,
    RecombinationOperator,
    RefinementOperator,
    OperatorResult,
    get_revision_operator,
    get_recombination_operator,
    get_refinement_operator
)

# Import SE-Darwin for actual code generation (Phase 5 integration)
try:
    import sys
    sys.path.insert(0, '/home/genesis/genesis-rebuild')
    from agents.se_darwin_agent import SEDarwinAgent
    SE_DARWIN_AVAILABLE = True
except ImportError:
    SEDarwinAgent = None
    SE_DARWIN_AVAILABLE = False

# OTEL observability
try:
    from opentelemetry import trace, metrics
    from opentelemetry.trace import Status, StatusCode
    tracer = trace.get_tracer(__name__)
    meter = metrics.get_meter(__name__)

    # Metrics for Solver Agent
    solver_trajectory_counter = meter.create_counter(
        "solver.trajectories.generated",
        description="Number of trajectories generated by Solver"
    )
    solver_feedback_counter = meter.create_counter(
        "solver.feedback.incorporated",
        description="Number of Verifier feedbacks incorporated"
    )
    solver_diversity_histogram = meter.create_histogram(
        "solver.diversity.score",
        description="Trajectory diversity scores"
    )
    solver_reward_histogram = meter.create_histogram(
        "solver.reward.computed",
        description="Solver reward scores"
    )
except ImportError:
    tracer = None
    meter = None
    solver_trajectory_counter = None
    solver_feedback_counter = None
    solver_diversity_histogram = None
    solver_reward_histogram = None

logger = get_logger("solver_agent")


@dataclass
class SolverConfig:
    """
    Solver Agent configuration.

    Based on arXiv:2510.23595 recommended weights:
    - diversity_weight: 0.3 (30% weight on trajectory diversity)
    - quality_weight: 0.5 (50% weight on benchmark score)
    - verifier_weight: 0.2 (20% weight on verifier difficulty)

    Attributes:
        diversity_weight: Weight for diversity score in reward calculation
        quality_weight: Weight for quality score in reward calculation
        verifier_weight: Weight for verifier challenge in reward calculation
        num_trajectories: Number of trajectories to generate per iteration
        max_iterations: Maximum evolution iterations before stopping
        diversity_threshold: Minimum diversity score to maintain (0-1)
        history_size: Number of recent trajectories to keep for diversity comparison
        baseline_strategy: Strategy for baseline trajectory generation
    """
    diversity_weight: float = 0.3
    quality_weight: float = 0.5
    verifier_weight: float = 0.2
    num_trajectories: int = 5
    max_iterations: int = 10
    diversity_threshold: float = 0.4
    history_size: int = 20
    baseline_strategy: str = "straightforward"  # "straightforward" | "exploratory" | "conservative"

    def __post_init__(self):
        """Validate configuration parameters."""
        total_weight = self.diversity_weight + self.quality_weight + self.verifier_weight
        if not (0.99 <= total_weight <= 1.01):  # Allow small floating point error
            logger.warning(f"Weights sum to {total_weight:.3f}, expected 1.0. Normalizing...")
            norm = total_weight
            self.diversity_weight /= norm
            self.quality_weight /= norm
            self.verifier_weight /= norm

        if self.num_trajectories < 2:
            raise ValueError(f"num_trajectories must be >= 2, got {self.num_trajectories}")

        if not 0.0 <= self.diversity_threshold <= 1.0:
            raise ValueError(f"diversity_threshold must be in [0,1], got {self.diversity_threshold}")


@dataclass
class SolverTrajectory:
    """
    Solution trajectory generated by Solver.

    Attributes:
        trajectory_id: Unique identifier for this trajectory
        code: Generated solution code
        reasoning: Reasoning steps for this solution
        generation_method: Operator used to generate ("baseline" | "revision" | "recombination" | "refinement")
        solver_confidence: Solver's confidence in this solution (0-1)
        diversity_score: Diversity score vs existing pool (0-1)
        timestamp: Creation timestamp
        metadata: Additional metadata (task info, iteration, etc.)
        parent_ids: IDs of parent trajectories (for recombination)
    """
    trajectory_id: str
    code: str
    reasoning: str
    generation_method: str
    solver_confidence: float
    diversity_score: float = 0.0
    timestamp: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)
    parent_ids: List[str] = field(default_factory=list)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "trajectory_id": self.trajectory_id,
            "code": self.code,
            "reasoning": self.reasoning,
            "generation_method": self.generation_method,
            "solver_confidence": self.solver_confidence,
            "diversity_score": self.diversity_score,
            "timestamp": self.timestamp,
            "metadata": self.metadata,
            "parent_ids": self.parent_ids
        }

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "SolverTrajectory":
        """Create from dictionary."""
        return cls(**data)


@dataclass
class VerifierFeedback:
    """
    Feedback received from Verifier agent.

    Attributes:
        trajectory_id: ID of evaluated trajectory
        verifier_score: Overall verifier score (0-1)
        correctness_score: Correctness evaluation (0-1)
        quality_score: Code quality evaluation (0-1)
        robustness_score: Robustness evaluation (0-1)
        generalization_score: Generalization evaluation (0-1)
        correctness_feedback: Textual feedback on correctness
        quality_feedback: Textual feedback on quality
        robustness_feedback: Textual feedback on robustness
        shortcuts_detected: List of detected shortcuts/patterns
        weak_areas: Areas where solution is weak
        timestamp: Feedback timestamp
    """
    trajectory_id: str
    verifier_score: float
    correctness_score: float
    quality_score: float
    robustness_score: float
    generalization_score: float
    correctness_feedback: str
    quality_feedback: str
    robustness_feedback: str
    shortcuts_detected: List[str] = field(default_factory=list)
    weak_areas: List[str] = field(default_factory=list)
    timestamp: float = field(default_factory=time.time)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "trajectory_id": self.trajectory_id,
            "verifier_score": self.verifier_score,
            "correctness_score": self.correctness_score,
            "quality_score": self.quality_score,
            "robustness_score": self.robustness_score,
            "generalization_score": self.generalization_score,
            "correctness_feedback": self.correctness_feedback,
            "quality_feedback": self.quality_feedback,
            "robustness_feedback": self.robustness_feedback,
            "shortcuts_detected": self.shortcuts_detected,
            "weak_areas": self.weak_areas,
            "timestamp": self.timestamp
        }


class SolverAgent:
    """
    Solver Agent generates solution trajectories in co-evolution loop.

    Based on arXiv:2510.23595 Section 3.1 "Solver Dynamics"

    The Solver's objective is to:
    1. Generate diverse solution trajectories
    2. Maximize solution quality (benchmark performance)
    3. Challenge the Verifier (adversarial learning)

    Co-evolution reward function:
        reward = w_q * quality + w_d * diversity + w_v * verifier_challenge

    where verifier_challenge = 1.0 - verifier_score (reward for fooling verifier)

    Integration with SE-Darwin:
    - Uses SE operators (Revision, Recombination, Refinement) for trajectory generation
    - Maintains TrajectoryPool for cross-iteration learning
    - Incorporates Verifier feedback for adversarial learning

    Attributes:
        agent_type: Type of agent being evolved (e.g., "qa_agent", "support_agent")
        config: Solver configuration parameters
        trajectory_pool: Pool for storing successful trajectories
        trajectory_history: Recent trajectories for diversity computation
        feedback_history: Recent Verifier feedback for learning
        revision_operator: SE-Darwin revision operator
        recombination_operator: SE-Darwin recombination operator
        refinement_operator: SE-Darwin refinement operator
        generation_count: Total trajectories generated
        feedback_incorporated_count: Total feedbacks incorporated
    """

    def __init__(
        self,
        agent_type: str,
        config: Optional[SolverConfig] = None,
        trajectory_pool: Optional[TrajectoryPool] = None,
        se_darwin_agent: Optional[Any] = None
    ):
        """
        Initialize Solver Agent.

        Args:
            agent_type: Type of agent being evolved (e.g., "qa_agent")
            config: Solver configuration (uses defaults if None)
            trajectory_pool: Trajectory pool for persistence (creates new if None)
            se_darwin_agent: Optional SE-Darwin agent for code generation (Phase 5)
        """
        self.agent_type = agent_type
        self.config = config or SolverConfig()
        self.trajectory_pool = trajectory_pool or get_trajectory_pool(agent_name=agent_type)

        # History tracking
        self.trajectory_history: List[SolverTrajectory] = []
        self.feedback_history: List[VerifierFeedback] = []

        # SE-Darwin operators (reuse existing infrastructure)
        self.revision_operator = get_revision_operator()
        self.recombination_operator = get_recombination_operator()
        self.refinement_operator = get_refinement_operator()

        # Phase 5: SE-Darwin integration for actual code generation
        self.se_darwin_agent = se_darwin_agent
        if SE_DARWIN_AVAILABLE and se_darwin_agent is None:
            try:
                self.se_darwin_agent = SEDarwinAgent(
                    agent_name=agent_type,
                    max_iterations=5,  # Fewer iterations for Solver variations
                    max_time_seconds=300  # 5 min timeout per trajectory
                )
                logger.info(f"Initialized SE-Darwin agent for code generation: {agent_type}")
            except Exception as e:
                logger.warning(f"Could not initialize SE-Darwin: {e}")
                self.se_darwin_agent = None

        # Metrics
        self.generation_count = 0
        self.feedback_incorporated_count = 0

        logger.info(
            f"Initialized SolverAgent for {agent_type} with config: "
            f"diversity_weight={self.config.diversity_weight:.2f}, "
            f"quality_weight={self.config.quality_weight:.2f}, "
            f"verifier_weight={self.config.verifier_weight:.2f}"
        )

    async def generate_trajectories(
        self,
        task: Dict[str, Any],
        verifier_feedback: Optional[List[VerifierFeedback]] = None
    ) -> List[SolverTrajectory]:
        """
        Generate solution trajectories for a given task.

        Algorithm (from arXiv:2510.23595 Algorithm 1):
        1. Generate baseline trajectory (straightforward approach)
        2. Generate N-1 diverse variations using SE-Darwin operators
        3. Incorporate Verifier feedback if available (adversarial learning)
        4. Compute diversity scores for all trajectories
        5. Return trajectories with metadata

        Args:
            task: Task specification dict with keys:
                - type: Task type (e.g., "code_generation", "validation")
                - description: Task description
                - constraints: Optional constraints
            verifier_feedback: Previous Verifier feedback for adversarial learning

        Returns:
            List of SolverTrajectory objects with generated solutions
        """
        span_ctx = tracer.start_span("solver.generate_trajectories") if tracer else None

        try:
            logger.info(
                f"Generating {self.config.num_trajectories} trajectories for "
                f"task type: {task.get('type', 'unknown')}"
            )

            trajectories: List[SolverTrajectory] = []

            # Step 1: Generate baseline trajectory
            baseline = await self._generate_baseline(task)
            trajectories.append(baseline)

            # Step 2: Generate diverse variations
            for i in range(self.config.num_trajectories - 1):
                trajectory = await self._generate_variation(
                    baseline=baseline,
                    task=task,
                    feedback=verifier_feedback,
                    iteration=i
                )
                trajectories.append(trajectory)

            # Step 3: Compute diversity scores
            for trajectory in trajectories:
                trajectory.diversity_score = self._compute_diversity_score(trajectory)

                # Update metrics
                if solver_diversity_histogram:
                    solver_diversity_histogram.record(
                        trajectory.diversity_score,
                        {"agent_type": self.agent_type, "method": trajectory.generation_method}
                    )

            # Step 4: Update history
            for trajectory in trajectories:
                self.update_history(trajectory)

            # Update metrics
            self.generation_count += len(trajectories)
            if solver_trajectory_counter:
                solver_trajectory_counter.add(
                    len(trajectories),
                    {"agent_type": self.agent_type}
                )

            logger.info(
                f"Generated {len(trajectories)} trajectories with diversity scores: "
                f"{[f'{t.diversity_score:.2f}' for t in trajectories]}"
            )

            if span_ctx:
                span_ctx.set_attribute("trajectory_count", len(trajectories))
                span_ctx.set_status(Status(StatusCode.OK))

            return trajectories

        except Exception as e:
            logger.error(f"Error generating trajectories: {e}", exc_info=True)
            if span_ctx:
                span_ctx.set_status(Status(StatusCode.ERROR, str(e)))
            raise
        finally:
            if span_ctx:
                span_ctx.end()

    async def _generate_baseline(self, task: Dict[str, Any]) -> SolverTrajectory:
        """
        Generate baseline trajectory (straightforward solution).

        Phase 5: Now uses SE-Darwin for actual code generation instead of placeholders.

        The baseline uses the configured strategy:
        - "straightforward": Direct, obvious solution
        - "exploratory": Novel, creative approach
        - "conservative": Safe, well-tested patterns

        Args:
            task: Task specification

        Returns:
            Baseline SolverTrajectory
        """
        logger.debug(f"Generating baseline trajectory with strategy: {self.config.baseline_strategy}")

        # Generate unique ID
        trajectory_id = f"solver_baseline_{self.agent_type}_{uuid.uuid4().hex[:8]}"

        # Baseline reasoning
        reasoning = f"Baseline {self.config.baseline_strategy} approach for {task.get('type', 'task')}"

        # Phase 5: Use SE-Darwin for actual code generation
        code = None
        if self.se_darwin_agent:
            try:
                problem_desc = task.get('description', '') or task.get('problem', '')
                if problem_desc:
                    logger.debug(f"Using SE-Darwin for baseline code generation: {problem_desc[:100]}...")
                    result = await self.se_darwin_agent.evolve_solution(
                        problem_description=problem_desc,
                        context=task
                    )
                    code = result.get('best_code', '')
                    reasoning += f"\nSE-Darwin evolution: {result.get('iterations_used', 0)} iterations"
                    logger.info(f"SE-Darwin generated baseline code ({len(code)} chars)")
            except Exception as e:
                logger.warning(f"SE-Darwin generation failed, using placeholder: {e}")

        # Fallback to placeholder if SE-Darwin unavailable or failed
        if not code:
            code = f"# Baseline {self.config.baseline_strategy} implementation\n# Task: {task.get('description', 'N/A')}\n"
            code += "# Phase 5 Note: SE-Darwin integration not available, using placeholder\n"

        # Baseline confidence (medium, not too high or low)
        solver_confidence = 0.6

        trajectory = SolverTrajectory(
            trajectory_id=trajectory_id,
            code=code,
            reasoning=reasoning,
            generation_method="baseline",
            solver_confidence=solver_confidence,
            metadata={
                "task_type": task.get("type"),
                "strategy": self.config.baseline_strategy,
                "agent_type": self.agent_type,
                "se_darwin_enabled": self.se_darwin_agent is not None
            }
        )

        logger.debug(f"Generated baseline trajectory: {trajectory_id}")
        return trajectory

    async def _generate_variation(
        self,
        baseline: SolverTrajectory,
        task: Dict[str, Any],
        feedback: Optional[List[VerifierFeedback]],
        iteration: int
    ) -> SolverTrajectory:
        """
        Generate trajectory variation with diversity and feedback incorporation.

        Strategy selection based on iteration and feedback:
        - Iteration 0-1: Use Revision operator (alternative approaches)
        - Iteration 2-3: Use Recombination operator (combine successful patterns)
        - Iteration 4+: Use Refinement operator (optimize existing solutions)

        If Verifier feedback exists:
        - Target weak areas identified by Verifier
        - Avoid detected shortcuts
        - Challenge Verifier's evaluation criteria

        Args:
            baseline: Baseline trajectory to build upon
            task: Task specification
            feedback: Verifier feedback for adversarial learning
            iteration: Variation iteration number

        Returns:
            Variation SolverTrajectory
        """
        logger.debug(f"Generating variation {iteration} with feedback: {feedback is not None}")

        # Select operator based on iteration
        if iteration < 2:
            operator_type = "revision"
        elif iteration < 4:
            operator_type = "recombination"
        else:
            operator_type = "refinement"

        # Generate unique ID
        trajectory_id = f"solver_var_{operator_type}_{iteration}_{uuid.uuid4().hex[:8]}"

        # If feedback exists, incorporate adversarial learning
        weak_areas = []
        shortcuts_to_avoid = []
        if feedback:
            for fb in feedback:
                weak_areas.extend(fb.weak_areas)
                shortcuts_to_avoid.extend(fb.shortcuts_detected)

        # Generate reasoning with feedback awareness
        reasoning = f"Variation {iteration} using {operator_type} operator"
        if weak_areas:
            reasoning += f"\nTargeting weak areas: {', '.join(set(weak_areas))}"
        if shortcuts_to_avoid:
            reasoning += f"\nAvoiding shortcuts: {', '.join(set(shortcuts_to_avoid))}"

        # Phase 5: Use SE-Darwin with operator guidance for code variation
        code = None
        if self.se_darwin_agent and baseline.code:
            try:
                # Build context with operator guidance and feedback
                problem_desc = task.get('description', '') or task.get('problem', '')
                context = dict(task)
                context['operator_type'] = operator_type
                context['baseline_code'] = baseline.code
                context['weak_areas'] = weak_areas
                context['shortcuts_to_avoid'] = shortcuts_to_avoid
                context['operator_instructions'] = self._get_operator_instructions(
                    operator_type, weak_areas, shortcuts_to_avoid
                )

                if problem_desc:
                    logger.debug(f"Using SE-Darwin for variation {iteration} ({operator_type})")
                    result = await self.se_darwin_agent.evolve_solution(
                        problem_description=problem_desc,
                        context=context
                    )
                    code = result.get('best_code', '')
                    reasoning += f"\nSE-Darwin {operator_type}: {result.get('iterations_used', 0)} iterations"
                    logger.info(f"SE-Darwin generated {operator_type} variation ({len(code)} chars)")
            except Exception as e:
                logger.warning(f"SE-Darwin {operator_type} failed, using placeholder: {e}")

        # Fallback to placeholder if SE-Darwin unavailable or failed
        if not code:
            code = f"# Variation {iteration} - {operator_type} operator\n"
            code += f"# Task: {task.get('description', 'N/A')}\n"
            if weak_areas:
                code += f"# Addressing: {', '.join(set(weak_areas[:3]))}\n"
            code += "# Phase 5 Note: SE-Darwin integration not available, using placeholder\n"

        # Confidence increases with successful feedback incorporation
        solver_confidence = 0.5 + (0.1 * len(weak_areas)) if weak_areas else 0.5
        solver_confidence = min(solver_confidence, 0.95)  # Cap at 0.95

        trajectory = SolverTrajectory(
            trajectory_id=trajectory_id,
            code=code,
            reasoning=reasoning,
            generation_method=operator_type,
            solver_confidence=solver_confidence,
            metadata={
                "task_type": task.get("type"),
                "iteration": iteration,
                "verifier_informed": feedback is not None,
                "weak_areas_targeted": len(weak_areas),
                "shortcuts_avoided": len(shortcuts_to_avoid),
                "agent_type": self.agent_type,
                "se_darwin_enabled": self.se_darwin_agent is not None
            },
            parent_ids=[baseline.trajectory_id]
        )

        logger.debug(f"Generated variation trajectory: {trajectory_id} ({operator_type})")
        return trajectory

    def _get_operator_instructions(
        self,
        operator_type: str,
        weak_areas: List[str],
        shortcuts_to_avoid: List[str]
    ) -> str:
        """
        Generate operator-specific instructions for SE-Darwin.

        Phase 5: Guides SE-Darwin to apply appropriate operator strategies.

        Args:
            operator_type: Operator to use (revision/recombination/refinement)
            weak_areas: Weak areas identified by Verifier
            shortcuts_to_avoid: Shortcuts detected by Verifier

        Returns:
            Operator-specific instructions string
        """
        instructions = ""

        if operator_type == "revision":
            instructions = "Apply REVISION operator: Generate alternative approach with different strategy.\n"
            if weak_areas:
                instructions += f"Focus on fixing these weak areas: {', '.join(weak_areas[:3])}\n"
            if shortcuts_to_avoid:
                instructions += f"Avoid these shortcuts: {', '.join(shortcuts_to_avoid[:3])}\n"

        elif operator_type == "recombination":
            instructions = "Apply RECOMBINATION operator: Combine successful patterns from baseline.\n"
            instructions += "Keep what works well, replace weak components with stronger alternatives.\n"
            if weak_areas:
                instructions += f"Replace these weak areas: {', '.join(weak_areas[:3])}\n"

        elif operator_type == "refinement":
            instructions = "Apply REFINEMENT operator: Optimize and polish existing solution.\n"
            instructions += "Improve code quality, performance, and robustness without changing core logic.\n"
            if weak_areas:
                instructions += f"Refine these areas: {', '.join(weak_areas[:3])}\n"

        return instructions

    def compute_solver_reward(
        self,
        trajectory: SolverTrajectory,
        benchmark_score: float,
        verifier_score: float
    ) -> float:
        """
        Compute Solver's reward for co-evolution.

        Based on arXiv:2510.23595 Equation 2 (Solver Reward Function):

        reward = w_q * quality + w_d * diversity + w_v * verifier_challenge

        where:
        - quality = benchmark_score (empirical validation)
        - diversity = diversity_score (vs existing pool)
        - verifier_challenge = 1.0 - verifier_score (reward for fooling verifier)

        Args:
            trajectory: Trajectory to evaluate
            benchmark_score: Empirical benchmark score (0-1)
            verifier_score: Verifier's evaluation score (0-1)

        Returns:
            Reward score (typically 0-1 range, can exceed 1.0 for exceptional solutions)
        """
        quality = benchmark_score
        diversity = trajectory.diversity_score
        verifier_challenge = 1.0 - verifier_score  # Higher reward for challenging verifier

        reward = (
            self.config.quality_weight * quality +
            self.config.diversity_weight * diversity +
            self.config.verifier_weight * verifier_challenge
        )

        logger.debug(
            f"Solver reward for {trajectory.trajectory_id}: "
            f"quality={quality:.3f}, diversity={diversity:.3f}, "
            f"challenge={verifier_challenge:.3f}, total={reward:.3f}"
        )

        # Update metrics
        if solver_reward_histogram:
            solver_reward_histogram.record(
                reward,
                {
                    "agent_type": self.agent_type,
                    "method": trajectory.generation_method
                }
            )

        return reward

    def _compute_diversity_score(self, trajectory: SolverTrajectory) -> float:
        """
        Measure trajectory diversity vs existing pool.

        Uses Jaccard distance for code similarity:
        diversity = 1 - average_similarity(trajectory, recent_pool)

        Higher diversity score = more different from existing solutions

        Args:
            trajectory: Trajectory to evaluate

        Returns:
            Diversity score (0-1, where 1 = maximally diverse)
        """
        if not self.trajectory_history:
            return 1.0  # Maximum diversity if pool empty

        # Compare against recent trajectories (sliding window)
        recent = self.trajectory_history[-min(len(self.trajectory_history), 5):]

        similarities = []
        for existing in recent:
            similarity = self._jaccard_similarity(
                trajectory.code,
                existing.code
            )
            similarities.append(similarity)

        # Diversity = 1 - average similarity
        avg_similarity = sum(similarities) / len(similarities) if similarities else 0.0
        diversity = 1.0 - avg_similarity

        logger.debug(
            f"Diversity score for {trajectory.trajectory_id}: {diversity:.3f} "
            f"(vs {len(recent)} recent trajectories)"
        )

        return diversity

    def _jaccard_similarity(self, code1: str, code2: str) -> float:
        """
        Compute Jaccard similarity between two code strings.

        Jaccard similarity = |intersection| / |union| of token sets

        Args:
            code1: First code string
            code2: Second code string

        Returns:
            Similarity score (0-1, where 1 = identical)
        """
        # Tokenize code by whitespace and punctuation
        tokens1 = set(code1.split())
        tokens2 = set(code2.split())

        if not tokens1 and not tokens2:
            return 1.0  # Both empty = identical

        intersection = tokens1 & tokens2
        union = tokens1 | tokens2

        if not union:
            return 0.0

        similarity = len(intersection) / len(union)
        return similarity

    def incorporate_feedback(
        self,
        feedback: VerifierFeedback
    ) -> None:
        """
        Incorporate Verifier feedback for adversarial learning.

        Feedback incorporation strategy:
        1. Store feedback in history
        2. Identify weak areas to address
        3. Identify shortcuts to avoid
        4. Update internal reward model (learning what Verifier values)

        Args:
            feedback: Verifier feedback to incorporate
        """
        logger.info(
            f"Incorporating feedback for trajectory {feedback.trajectory_id}: "
            f"score={feedback.verifier_score:.2f}, "
            f"weak_areas={len(feedback.weak_areas)}, "
            f"shortcuts={len(feedback.shortcuts_detected)}"
        )

        self.feedback_history.append(feedback)

        # Keep only recent feedback (sliding window)
        if len(self.feedback_history) > self.config.history_size:
            self.feedback_history = self.feedback_history[-self.config.history_size:]

        self.feedback_incorporated_count += 1

        # Update metrics
        if solver_feedback_counter:
            solver_feedback_counter.add(1, {"agent_type": self.agent_type})

    def update_history(self, trajectory: SolverTrajectory) -> None:
        """
        Add trajectory to history for diversity computation.

        Maintains a sliding window of recent trajectories to:
        - Compute diversity scores
        - Track generation patterns
        - Enable cross-iteration learning

        Args:
            trajectory: Trajectory to add to history
        """
        self.trajectory_history.append(trajectory)

        # Keep only recent trajectories (sliding window)
        if len(self.trajectory_history) > self.config.history_size:
            self.trajectory_history = self.trajectory_history[-self.config.history_size:]

        logger.debug(
            f"Updated history with {trajectory.trajectory_id}, "
            f"pool size: {len(self.trajectory_history)}/{self.config.history_size}"
        )

    def get_statistics(self) -> Dict[str, Any]:
        """
        Get Solver Agent statistics.

        Returns:
            Statistics dict with generation counts, feedback incorporation, diversity metrics
        """
        avg_diversity = (
            sum(t.diversity_score for t in self.trajectory_history) / len(self.trajectory_history)
            if self.trajectory_history else 0.0
        )

        avg_confidence = (
            sum(t.solver_confidence for t in self.trajectory_history) / len(self.trajectory_history)
            if self.trajectory_history else 0.0
        )

        return {
            "agent_type": self.agent_type,
            "generation_count": self.generation_count,
            "feedback_incorporated_count": self.feedback_incorporated_count,
            "history_size": len(self.trajectory_history),
            "feedback_history_size": len(self.feedback_history),
            "average_diversity": avg_diversity,
            "average_confidence": avg_confidence,
            "config": {
                "diversity_weight": self.config.diversity_weight,
                "quality_weight": self.config.quality_weight,
                "verifier_weight": self.config.verifier_weight,
                "num_trajectories": self.config.num_trajectories
            }
        }


# Factory function for easy instantiation
def get_solver_agent(
    agent_type: str,
    config: Optional[SolverConfig] = None,
    trajectory_pool: Optional[TrajectoryPool] = None
) -> SolverAgent:
    """
    Factory function to create SolverAgent instance.

    Args:
        agent_type: Type of agent being evolved
        config: Solver configuration (uses defaults if None)
        trajectory_pool: Trajectory pool (creates new if None)

    Returns:
        Initialized SolverAgent
    """
    return SolverAgent(
        agent_type=agent_type,
        config=config,
        trajectory_pool=trajectory_pool
    )
