"""
End-to-end tests for Genesis Meta-Agent business creation.

Tests autonomous business creation across all 10 archetypes with:
- HTDAG task decomposition
- HALO agent routing
- Swarm team composition
- WaltzRL safety validation
- LangGraph memory integration

Version: 1.0
Date: November 3, 2025
Author: Cora (Agent Orchestration Specialist)
"""

import pytest
import asyncio
from unittest.mock import Mock, AsyncMock, patch
from typing import Dict, List, Any

from infrastructure.genesis_meta_agent import (
    GenesisMetaAgent,
    BusinessRequirements,
    BusinessCreationStatus,
    BusinessCreationResult,
    BusinessCreationError
)
from infrastructure.genesis_business_types import (
    get_business_archetype,
    get_all_archetypes,
    DifficultyLevel,
    validate_business_type
)
from infrastructure.task_dag import TaskDAG, Task, TaskStatus


@pytest.fixture
def genesis_meta_agent():
    """
    Create GenesisMetaAgent instance for testing.

    Disables external dependencies (LLM, MongoDB) for unit tests.
    """
    with patch('infrastructure.genesis_meta_agent.GenesisLangGraphStore'):
        with patch('infrastructure.genesis_meta_agent.WaltzRLSafety'):
            agent = GenesisMetaAgent(
                mongodb_uri="mongodb://localhost:27017/test",
                enable_safety=False,  # Disable for faster tests
                enable_memory=False,  # Disable for faster tests
                autonomous=True
            )
            return agent


@pytest.fixture(autouse=True)
def patch_openai_client_default():
    """
    Provide a default OpenAI client stub so tests do not require real API keys.
    Individual tests override this patch as needed.
    """
    with patch('infrastructure.genesis_meta_agent.OpenAIClient') as mock_class:
        client = AsyncMock()
        client.generate_structured_output = AsyncMock(return_value={
            "name": "Default Test Business",
            "description": "Autogenerated description",
            "target_audience": "General users",
            "monetization": "Subscription",
            "mvp_features": ["Feature A", "Feature B"],
            "tech_stack": ["Next.js", "Python"],
            "success_metrics": {"first_user": "< 24h"}
        })
        mock_class.return_value = client
        yield


@pytest.fixture
def mock_llm_client():
    """Mock LLM client for business idea generation"""
    mock = AsyncMock()
    mock.generate_structured_output.return_value = {
        "name": "Test SaaS Tool",
        "description": "An automated testing tool for developers",
        "target_audience": "Software developers and QA teams",
        "monetization": "Freemium with $19/month premium tier",
        "mvp_features": [
            "Automated test generation",
            "CI/CD integration",
            "Test result analytics"
        ],
        "tech_stack": [
            "Next.js",
            "Python",
            "OpenAI API",
            "Vercel"
        ],
        "success_metrics": {
            "first_user": "< 48 hours",
            "conversion_rate": "> 2%"
        }
    }
    return mock


class TestGenesisMetaAgentInitialization:
    """Test GenesisMetaAgent initialization and configuration"""

    def test_initialization_default_config(self):
        """Test initialization with default configuration"""
        with patch('infrastructure.genesis_meta_agent.GenesisLangGraphStore'):
            with patch('infrastructure.genesis_meta_agent.WaltzRLSafety'):
                agent = GenesisMetaAgent()

                assert agent.htdag is not None
                assert agent.halo is not None
                assert agent.swarm_class is not None
                assert agent.autonomous is True

    def test_initialization_custom_config(self):
        """Test initialization with custom configuration"""
        with patch('infrastructure.genesis_meta_agent.GenesisLangGraphStore'):
            with patch('infrastructure.genesis_meta_agent.WaltzRLSafety'):
                agent = GenesisMetaAgent(
                    mongodb_uri="mongodb://custom:27017",
                    enable_safety=False,
                    enable_memory=False,
                    autonomous=False
                )

                assert agent.autonomous is False
                assert agent.safety is None
                assert agent.memory is None


class TestBusinessIdeaGeneration:
    """Test business idea generation with GPT-4o"""

    @pytest.mark.asyncio
    async def test_generate_business_idea_saas(self, genesis_meta_agent, mock_llm_client):
        """Test generating a SaaS business idea"""
        with patch('infrastructure.genesis_meta_agent.OpenAIClient', return_value=mock_llm_client):
            requirements = await genesis_meta_agent._generate_business_idea("saas_tool")

            assert isinstance(requirements, BusinessRequirements)
            assert requirements.name == "Test SaaS Tool"
            assert len(requirements.mvp_features) > 0
            assert len(requirements.tech_stack) > 0
            assert requirements.business_type == "saas_tool"

    @pytest.mark.asyncio
    async def test_generate_business_idea_with_memory(self, genesis_meta_agent, mock_llm_client):
        """Test business idea generation with memory context"""
        # Enable memory for this test
        genesis_meta_agent.memory = Mock()
        genesis_meta_agent.memory.search = AsyncMock(return_value=[
            Mock(value={"name": "Previous SaaS", "tech_stack": ["Next.js"]})
        ])

        with patch('infrastructure.genesis_meta_agent.OpenAIClient', return_value=mock_llm_client):
            requirements = await genesis_meta_agent._generate_business_idea("saas_tool")

            # Verify memory was queried
            genesis_meta_agent.memory.search.assert_called_once()
            assert requirements.name is not None


class TestTeamComposition:
    """Test team composition with Swarm Optimizer"""

    @pytest.mark.asyncio
    async def test_compose_team_saas(self, genesis_meta_agent):
        """Test composing team for SaaS tool"""
        requirements = BusinessRequirements(
            name="Test SaaS",
            description="A test SaaS application",
            target_audience="Developers",
            monetization="Subscription",
            mvp_features=["Feature 1", "Feature 2"],
            tech_stack=["Next.js", "Python", "Stripe"],
            success_metrics={"first_user": "< 48h"}
        )

        # Team composition is now capability-based, no swarm mock needed
        team = await genesis_meta_agent._compose_team(requirements, [])

        assert len(team) >= 3  # Minimum team size (builder, deploy, qa)
        assert "builder_agent" in team  # Always need builder
        assert "deploy_agent" in team  # Always need deploy

    @pytest.mark.asyncio
    async def test_extract_required_capabilities(self, genesis_meta_agent):
        """Test extracting required capabilities from requirements"""
        requirements = BusinessRequirements(
            name="Python Tool",
            description="A Python-based tool",
            target_audience="Developers",
            monetization="Free",
            mvp_features=["CLI interface"],
            tech_stack=["Python", "pytest", "Stripe"],
            success_metrics={}
        )

        capabilities = genesis_meta_agent._extract_required_capabilities(requirements)

        assert "python" in capabilities
        assert "testing" in capabilities
        assert "payments" in capabilities


class TestTaskDecomposition:
    """Test task decomposition with HTDAG"""

    @pytest.mark.asyncio
    async def test_decompose_business_tasks(self, genesis_meta_agent):
        """Test decomposing business into hierarchical tasks"""
        requirements = BusinessRequirements(
            name="Test Business",
            description="A test business",
            target_audience="Users",
            monetization="Freemium",
            mvp_features=["Feature 1", "Feature 2"],
            tech_stack=["Next.js", "MongoDB"],
            success_metrics={"deploy": "< 4h"}
        )

        # Mock HTDAG decomposition
        mock_dag = TaskDAG()
        mock_dag.add_task(Task(
            task_id="task_1",
            description="Implement frontend",
            status=TaskStatus.PENDING
        ))
        mock_dag.add_task(Task(
            task_id="task_2",
            description="Deploy to production",
            status=TaskStatus.PENDING
        ))

        with patch.object(genesis_meta_agent.htdag, 'decompose_task', return_value=mock_dag):
            dag = await genesis_meta_agent._decompose_business_tasks(requirements)

            assert len(dag.tasks) >= 2
            # Check task exists
            assert "task_1" in [t.task_id for t in dag.tasks.values()]


class TestTaskRouting:
    """Test task routing with HALO"""

    @pytest.mark.asyncio
    async def test_route_tasks(self, genesis_meta_agent):
        """Test routing tasks to agents"""
        # Create mock DAG
        dag = TaskDAG()
        dag.add_task(Task(task_id="task_1", description="Build frontend"))
        dag.add_task(Task(task_id="task_2", description="Deploy app"))

        team = ["builder_agent", "deploy_agent"]

        # Mock HALO routing
        from infrastructure.halo_router import RoutingPlan
        mock_plan = RoutingPlan(
            assignments={
                "task_1": "builder_agent",
                "task_2": "deploy_agent"
            },
            explanations={
                "task_1": "Builder handles frontend",
                "task_2": "Deploy handles deployment"
            }
        )

        with patch.object(genesis_meta_agent.halo, 'route_tasks', return_value=mock_plan):
            routing_plan = await genesis_meta_agent._route_tasks(dag, team)

            assert len(routing_plan.assignments) == 2
            assert routing_plan.assignments["task_1"] == "builder_agent"
            assert routing_plan.assignments["task_2"] == "deploy_agent"


class TestSafetyValidation:
    """Test safety validation with WaltzRL"""

    @pytest.mark.asyncio
    async def test_validate_task_safety_safe(self, genesis_meta_agent):
        """Test safety validation for safe task"""
        task = Task(task_id="task_1", description="Deploy to Vercel")

        # Enable safety for this test
        genesis_meta_agent.safety = Mock()
        genesis_meta_agent.safety.filter_unsafe_query = Mock(return_value=(True, None, "Safe"))

        result = await genesis_meta_agent._validate_task_safety(task, "deploy_agent", True)

        assert result["safe"] is True

    @pytest.mark.asyncio
    async def test_validate_task_safety_unsafe_autonomous(self, genesis_meta_agent):
        """Test safety validation blocks unsafe task in autonomous mode"""
        task = Task(task_id="task_1", description="Delete all production data")

        # Enable safety for this test
        from infrastructure.waltzrl_safety import SafetyScore, SafetyClassification
        mock_score = SafetyScore(
            classification=SafetyClassification.UNSAFE,
            confidence=0.95,
            safety_score=0.1,
            helpfulness_score=0.0,
            reasoning="Destructive operation"
        )
        genesis_meta_agent.safety = Mock()
        genesis_meta_agent.safety.filter_unsafe_query = Mock(
            return_value=(False, mock_score, "Unsafe operation")
        )

        result = await genesis_meta_agent._validate_task_safety(task, "builder_agent", autonomous=True)

        assert result["safe"] is False
        assert "Safety violation" in result["reason"]


class TestBusinessCreation:
    """Test end-to-end business creation"""

    @pytest.mark.asyncio
    async def test_create_business_saas_tool_with_requirements(self, genesis_meta_agent):
        """Test creating SaaS tool with custom requirements"""
        requirements = BusinessRequirements(
            name="AI Writing Assistant",
            description="Help users improve their writing with AI suggestions",
            target_audience="Writers and content creators",
            monetization="Freemium",
            mvp_features=["Text input", "AI suggestions", "Premium features"],
            tech_stack=["Next.js", "OpenAI API", "Stripe"],
            success_metrics={"first_user": "< 24h"}
        )

        # Mock all subsystems
        with patch.object(genesis_meta_agent, '_compose_team', return_value=["builder_agent", "deploy_agent"]):
            with patch.object(genesis_meta_agent, '_decompose_business_tasks') as mock_decompose:
                mock_dag = TaskDAG()
                mock_dag.add_task(Task(task_id="task_1", description="Build"))
                mock_decompose.return_value = mock_dag

                with patch.object(genesis_meta_agent, '_route_tasks') as mock_route:
                    from infrastructure.halo_router import RoutingPlan
                    mock_route.return_value = RoutingPlan(
                        assignments={"task_1": "builder_agent"}
                    )

                    with patch.object(genesis_meta_agent, '_execute_tasks') as mock_execute:
                        mock_execute.return_value = [
                            {
                                "task_id": "task_1",
                                "status": "completed",
                                "deployment_url": "https://test.vercel.app"
                            }
                        ]

                        result = await genesis_meta_agent.create_business(
                            business_type="saas_tool",
                            requirements=requirements,
                            enable_memory_learning=False
                        )

                        assert result.success is True
                        assert result.status == BusinessCreationStatus.SUCCESS
                        assert len(result.team_composition) >= 2
                        assert result.deployment_url is not None
                        assert result.revenue_projection["status"] == "projected"
                        assert result.revenue_projection["projected_monthly_revenue"] > 0
                        assert result.revenue_projection["confidence"] >= 0.55

    @pytest.mark.asyncio
    async def test_create_business_autogenerate_idea(self, genesis_meta_agent, mock_llm_client):
        """Test creating business with auto-generated idea"""
        with patch('infrastructure.genesis_meta_agent.OpenAIClient', return_value=mock_llm_client):
            with patch.object(genesis_meta_agent, '_compose_team', return_value=["builder_agent"]):
                with patch.object(genesis_meta_agent, '_decompose_business_tasks') as mock_decompose:
                    mock_dag = TaskDAG()
                    mock_dag.add_task(Task(task_id="task_1", description="Build"))
                    mock_decompose.return_value = mock_dag

                    with patch.object(genesis_meta_agent, '_route_tasks') as mock_route:
                        from infrastructure.halo_router import RoutingPlan
                        mock_route.return_value = RoutingPlan(assignments={"task_1": "builder_agent"})

                        with patch.object(genesis_meta_agent, '_execute_tasks') as mock_execute:
                            mock_execute.return_value = [{"task_id": "task_1", "status": "completed"}]

                            result = await genesis_meta_agent.create_business(
                                business_type="saas_tool",
                                requirements=None,  # Auto-generate
                                enable_memory_learning=False
                            )

                            assert result.requirements.name == "Test SaaS Tool"
                            mock_llm_client.generate_structured_output.assert_called_once()
                            assert result.revenue_projection["projected_monthly_revenue"] > 0
                            assert result.revenue_projection["status"] == "projected"


class TestBusinessArchetypes:
    """Test business archetype integration"""

    def test_all_archetypes_have_valid_structure(self):
        """Test that all 10 archetypes have required fields"""
        archetypes = get_all_archetypes()

        assert len(archetypes) == 10

        for archetype in archetypes:
            assert archetype.type is not None
            assert archetype.name is not None
            assert len(archetype.typical_features) > 0
            assert len(archetype.tech_stack) > 0
            assert len(archetype.required_agents) > 0
            assert len(archetype.success_metrics) > 0

    def test_validate_business_type(self):
        """Test business type validation"""
        assert validate_business_type("saas_tool") is True
        assert validate_business_type("content_website") is True
        assert validate_business_type("invalid_type") is False

    @pytest.mark.parametrize("business_type", [
        "saas_tool",
        "content_website",
        "ecommerce_store",
        "landing_page_waitlist",
        "saas_dashboard",
        "marketplace",
        "ai_chatbot_service",
        "api_service",
        "newsletter_automation",
        "no_code_tool"
    ])
    def test_archetype_retrieval(self, business_type):
        """Test retrieving each business archetype"""
        archetype = get_business_archetype(business_type)

        assert archetype is not None
        assert archetype.type == business_type


class TestSuccessDetection:
    """Test business creation success detection"""

    def test_is_successful_all_completed(self, genesis_meta_agent):
        """Test success detection with all tasks completed"""
        results = [
            {"task_id": "1", "status": "completed"},
            {"task_id": "2", "status": "completed"},
            {"task_id": "3", "status": "completed"}
        ]

        assert genesis_meta_agent._is_successful(results) is True

    def test_is_successful_with_blocked_task(self, genesis_meta_agent):
        """Test failure detection with blocked task"""
        results = [
            {"task_id": "1", "status": "completed"},
            {"task_id": "2", "status": "blocked", "reason": "Safety violation"}
        ]

        assert genesis_meta_agent._is_successful(results) is False

    def test_is_successful_low_completion_rate(self, genesis_meta_agent):
        """Test failure detection with low completion rate"""
        results = [
            {"task_id": "1", "status": "completed"},
            {"task_id": "2", "status": "failed"},
            {"task_id": "3", "status": "failed"},
            {"task_id": "4", "status": "failed"},
            {"task_id": "5", "status": "failed"}
        ]

        # Only 20% completion (1/5)
        assert genesis_meta_agent._is_successful(results) is False


class TestMemoryIntegration:
    """Test memory integration for pattern learning"""

    @pytest.mark.asyncio
    async def test_store_success_pattern(self, genesis_meta_agent):
        """Test storing successful business pattern in memory"""
        # Enable memory for this test
        genesis_meta_agent.memory = Mock()
        genesis_meta_agent.memory.put = AsyncMock()

        requirements = BusinessRequirements(
            name="Test Business",
            description="Test",
            target_audience="Users",
            monetization="Free",
            mvp_features=["Feature"],
            tech_stack=["Next.js"],
            success_metrics={}
        )

        team = ["builder_agent", "deploy_agent"]
        task_results = [{"task_id": "1", "status": "completed"}]

        await genesis_meta_agent._store_success_pattern(
            business_id="test-123",
            business_type="saas_tool",
            requirements=requirements,
            team=team,
            task_results=task_results
        )

        genesis_meta_agent.memory.put.assert_called_once()

    @pytest.mark.asyncio
    async def test_query_similar_businesses(self, genesis_meta_agent):
        """Test querying memory for similar businesses"""
        # Enable memory for this test
        genesis_meta_agent.memory = Mock()
        genesis_meta_agent.memory.search = AsyncMock(return_value=[
            Mock(value={"business_type": "saas_tool", "success": True})
        ])

        similar = await genesis_meta_agent._query_similar_businesses("saas_tool")

        assert len(similar) == 1
        assert similar[0]["business_type"] == "saas_tool"


class TestErrorHandling:
    """Test error handling and edge cases"""

    @pytest.mark.asyncio
    async def test_create_business_handles_llm_error(self, genesis_meta_agent):
        """Test graceful handling of LLM generation error"""
        from infrastructure.llm_client import LLMClientError

        with patch('infrastructure.genesis_meta_agent.OpenAIClient') as mock_class:
            mock_client = AsyncMock()
            mock_client.generate_structured_output.side_effect = LLMClientError("API Error")
            mock_class.return_value = mock_client

            with pytest.raises(LLMClientError):
                await genesis_meta_agent._generate_business_idea("saas_tool")

    @pytest.mark.asyncio
    async def test_create_business_handles_exception(self, genesis_meta_agent):
        """Test error handling for unexpected exceptions"""
        with patch.object(genesis_meta_agent, '_generate_business_idea', side_effect=Exception("Unexpected error")):
            result = await genesis_meta_agent.create_business(
                business_type="saas_tool",
                requirements=None
            )

            assert result.status == BusinessCreationStatus.FAILED
            assert result.error_message == "Unexpected error"
            assert result.success is False
            assert result.revenue_projection["projected_monthly_revenue"] == 0
            assert result.revenue_projection["status"] == "unavailable"


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
